import google.generativeai as genai
import os
from datetime import datetime
import pickle
import faiss
from sentence_transformers import SentenceTransformer

# --- åƒæ•¸å€ ---
USE_FAISS = True  # âœ… é–‹é—œï¼šæ˜¯å¦ä½¿ç”¨å‘é‡çŸ¥è­˜åº«è¼”åŠ©å›ç­”

# --- åˆå§‹åŒ– Gemini ---
api_key_path = "api-key.txt"
if not os.path.exists(api_key_path):
    raise FileNotFoundError(f"æ‰¾ä¸åˆ° API é‡‘é‘°æª”æ¡ˆï¼š{api_key_path}")
with open(api_key_path, "r", encoding="utf-8") as f:
    api_key = f.read().strip()

genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.0-flash")
chat = model.start_chat()

# --- è®€å–å‘é‡åº« ---
if USE_FAISS:
    index = faiss.read_index("faiss_index.index")
    with open("doc_sources.pkl", "rb") as f:
        data = pickle.load(f)
    docs = data["docs"]
    sources = data["sources"]
    embedder = SentenceTransformer("paraphrase-multilingual-mpnet-base-v2")
    # embedder = SentenceTransformer("all-MiniLM-L6-v2")

# --- æª”æ¡ˆå„²å­˜è¨­å®š ---
start_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
history_filename = f"chat_history_{start_time}.txt"

def chat_with_gemini(user_input):
    if USE_FAISS:
        query_vector = embedder.encode([user_input], convert_to_numpy=True)

        if query_vector.ndim == 1:
            query_vector = query_vector.reshape(1, -1)

        assert query_vector.shape[1] == index.d, f"âŒ ç¶­åº¦éŒ¯èª¤ï¼æŸ¥è©¢å‘é‡ç‚º {query_vector.shape[1]}ï¼Œç´¢å¼•ç‚º {index.d}"

        top_k = 10  # å¯ä»¥èª¿æ•´æˆä½ å¸Œæœ›çš„è¿”å›æ•¸é‡
        D, I = index.search(query_vector, top_k)

        # è¨­å®šè·é›¢é–¾å€¼ï¼ˆL2è·é›¢è¶Šå°è¶Šç›¸ä¼¼ï¼‰
        threshold = 0.75
        valid_results = [(docs[i], sources[i], d) for i, d in zip(I[0], D[0]) if d < threshold]

        if valid_results:
            retrieved_chunks = [f"[{src}] {chunk}" for chunk, src, _ in valid_results]
            context = "\n".join(retrieved_chunks)
            prompt = f"ä½ æ˜¯ä¸€å€‹è°æ˜çš„ AI åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼š\n\n{context}\n\nå•é¡Œï¼š{user_input}"
        else:
            # æ²’æœ‰æ‰¾åˆ°è¶³å¤ ç›¸ä¼¼çš„æ®µè½ï¼Œæ”¹ç”¨ fallback æ¨¡å¼
            prompt = f"""æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ã€‚è«‹ä¾ä½ è‡ªå·±çš„çŸ¥è­˜å›ç­”ä»¥ä¸‹å•é¡Œï¼š
å•é¡Œï¼š{user_input}"""
    else:
        prompt = user_input

    # å‚³é€è¨Šæ¯çµ¦ Gemini
    response = chat.send_message(prompt)
    ai_reply = response.text

    # å„²å­˜å°è©±æ­·å²
    with open(history_filename, "a", encoding="utf-8") as f:
        f.write(f"ä½ ï¼š{user_input}\n\n")
        f.write(f"Geminiï¼š{ai_reply}\n\n")

    return ai_reply

if __name__ == "__main__":
    print("ğŸ¤– Gemini Chat CLI å·²å•Ÿå‹•ï¼ˆè¼¸å…¥ 'exit' æˆ– 'quit' é›¢é–‹ï¼‰")
    if USE_FAISS:
        print("ğŸ“š å·²å•Ÿç”¨çŸ¥è­˜åº«æŸ¥è©¢æ¨¡å¼\n")
    else:
        print("ğŸ’¬ ä½¿ç”¨ç´” LLM æ¨¡å¼ï¼ˆæœªå•Ÿç”¨çŸ¥è­˜åº«ï¼‰\n")

    while True:
        user_input = input("ä½ ï¼š")
        if user_input.strip().lower() == "exit" or user_input.strip().lower() == "quit":
            print(f"ğŸ“„ å°è©±å·²å„²å­˜ç‚ºï¼š{history_filename}")
            print("ğŸ‘‹ å†è¦‹ï¼")
            break
        reply = chat_with_gemini(user_input)
        print("Geminiï¼š", reply)

