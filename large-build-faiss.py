import os
import json
import faiss
import pickle
import fitz  # PyMuPDF
import pandas as pd
from docx import Document
from sentence_transformers import SentenceTransformer

# --- è¨­å®šå€ ---
KNOWLEDGE_DIR = "knowledge_files"
CHUNK_SIZE = 250
CHUNK_OVERLAP = 50
INDEX_PATH = "faiss_index.index"
SOURCE_PATH = "doc_sources.pkl"
EMBEDDING_MODEL = "paraphrase-multilingual-mpnet-base-v2"

# --- åˆå§‹åŒ–æ¨¡å‹ ---
embedder = SentenceTransformer(EMBEDDING_MODEL)

# --- è³‡æ–™å„²å­˜å€ ---
docs = []
sources = []

# --- å„é¡å‹è®€å–å‡½å¼ ---
def read_pdf(path):
    with fitz.open(path) as pdf:
        return [line for page in pdf for line in page.get_text().split("\n") if line.strip()]

def read_docx(path):
    return [para.text for para in Document(path).paragraphs if para.text.strip()]

# def read_csv(path):
#     return pd.read_csv(path).astype(str).values.flatten().tolist()

def read_csv(path):
    df = pd.read_csv(path)
    lines = []
    for i, row in df.iterrows():
        description = ", ".join(f"{col.strip()}ï¼š{str(val).strip()}" for col, val in row.items())
        lines.append(f"ç¬¬ {i+1} ç­†è³‡æ–™ï¼š{description}")
    return lines

def read_txt(path):
    with open(path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]

def read_json(path):
    def extract_text(obj):
        if isinstance(obj, dict):
            return sum([extract_text(v) for v in obj.values()], [])
        elif isinstance(obj, list):
            return sum([extract_text(i) for i in obj], [])
        elif isinstance(obj, str):
            return [obj]
        return []
    with open(path, "r", encoding="utf-8") as f:
        return extract_text(json.load(f))

def read_md(path):
    return read_txt(path)

READER_FUNCS = {
    ".pdf": read_pdf,
    ".docx": read_docx,
    ".csv": read_csv,
    ".txt": read_txt,
    ".json": read_json,
    ".md": read_md,
}

def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end].strip())
        start += chunk_size - overlap
    return chunks

# --- è®€å–ä¸¦è™•ç†è³‡æ–™ ---
for filename in os.listdir(KNOWLEDGE_DIR):
    path = os.path.join(KNOWLEDGE_DIR, filename)
    ext = os.path.splitext(filename)[1].lower()

    if ext not in READER_FUNCS:
        print(f"! ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{filename}")
        continue

    lines = READER_FUNCS[ext](path)

    for line in lines:
        for chunk in chunk_text(line):
            if chunk:
                docs.append(chunk)
                sources.append(filename)

# --- å‘é‡åŒ–ä¸¦å»ºç«‹ç´¢å¼• ---
print("ğŸ“¦ æ­£åœ¨ç·¨ç¢¼å‘é‡...")
doc_embeddings = embedder.encode(docs, convert_to_numpy=True, show_progress_bar=True)

d = doc_embeddings.shape[1]  # å‘é‡ç¶­åº¦
nlist = 500  # èšé¡ä¸­å¿ƒæ•¸é‡ï¼Œå¯ä¾è³‡æ–™é‡èª¿æ•´ï¼Œè³‡æ–™è¶Šå¤šnlistè¶Šå¤§

quantizer = faiss.IndexFlatL2(d)  # ç”¨ä½œèšé¡ä¸­å¿ƒçš„ç²¾ç¢ºç´¢å¼•
index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)

# è¨“ç·´ç´¢å¼• (å¿…é ˆå…ˆå‘¼å«trainï¼Œä¸¦ä¸”è³‡æ–™ç­†æ•¸éœ€å¤§æ–¼nlist)
assert doc_embeddings.shape[0] > nlist, "è³‡æ–™ç­†æ•¸å¿…é ˆå¤§æ–¼èšé¡æ•¸ nlist"

print("âš™ï¸ æ­£åœ¨è¨“ç·´ç´¢å¼•...")
index.train(doc_embeddings)

print("â• æ­£åœ¨åŠ å…¥å‘é‡...")
index.add(doc_embeddings)

# --- å„²å­˜ç´¢å¼•èˆ‡å°æ‡‰ä¾†æº ---
faiss.write_index(index, INDEX_PATH)
with open(SOURCE_PATH, "wb") as f:
    pickle.dump({"docs": docs, "sources": sources}, f)

print("âœ… å‘é‡ç´¢å¼•å»ºç«‹èˆ‡å„²å­˜å®Œæˆï¼")

